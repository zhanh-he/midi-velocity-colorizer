{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 02:55:16.504439: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-02-14 02:55:16.535490: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-02-14 02:55:16.535531: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-02-14 02:55:16.536439: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-02-14 02:55:16.543230: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-02-14 02:55:17.116316: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from keras.models import load_model\n",
    "from keras.losses import mse, cosine_similarity\n",
    "\n",
    "# Paths\n",
    "DATASET_PATH = 'converted_dataset/maestro-v3/test'\n",
    "OUTPUT_PATH = 'converted_dataset/maestro-v3//test_predict'\n",
    "MODEL_PATH = 'saved_models/mvi-v2-2023-07-20_13-00_56-h4-e5-mse_cosine_loss-alpha0.15-m0.60-LSTM-luong_attention-MAESTRO.h5'\n",
    "\n",
    "# Constants\n",
    "SAMPLE_LENGTH = 4\n",
    "columns_train = ['time_diff', 'note_num', 'length', 'note_num_diff', 'low_octave']\n",
    "columns_label = ['velocity']\n",
    "columns_full_input = ['time', 'time_diff', 'note_num', 'length', 'note_num_diff', 'low_octave']\n",
    "FLOAT_DTYPE = np.float32\n",
    "FEATURE_NUM = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_4 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm_5 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-02-14 02:55:24.908542: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 02:55:24.943134: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 02:55:24.944170: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 02:55:24.948685: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 02:55:24.949785: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 02:55:24.950494: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 02:55:25.056914: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 02:55:25.057760: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 02:55:25.057777: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2022] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2025-02-14 02:55:25.058529: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:887] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2025-02-14 02:55:25.058565: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13687 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3080 Ti Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                Output Shape                 Param #   Connected to                  \n",
      "==================================================================================================\n",
      " input_5 (InputLayer)        [(None, 4, 5)]               0         []                            \n",
      "                                                                                                  \n",
      " lstm_4 (LSTM)               [(None, 4, 4),               160       ['input_5[0][0]']             \n",
      "                              (None, 4),                                                          \n",
      "                              (None, 4)]                                                          \n",
      "                                                                                                  \n",
      " batch_normalization_6 (Bat  (None, 4)                    16        ['lstm_4[0][1]']              \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " repeat_vector_2 (RepeatVec  (None, 4, 4)                 0         ['batch_normalization_6[0][0]'\n",
      " tor)                                                               ]                             \n",
      "                                                                                                  \n",
      " batch_normalization_7 (Bat  (None, 4)                    16        ['lstm_4[0][2]']              \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " lstm_5 (LSTM)               (None, 4, 4)                 144       ['repeat_vector_2[0][0]',     \n",
      "                                                                     'batch_normalization_6[0][0]'\n",
      "                                                                    , 'batch_normalization_7[0][0]\n",
      "                                                                    ']                            \n",
      "                                                                                                  \n",
      " dot_4 (Dot)                 (None, 4, 4)                 0         ['lstm_5[0][0]',              \n",
      "                                                                     'lstm_4[0][0]']              \n",
      "                                                                                                  \n",
      " activation_2 (Activation)   (None, 4, 4)                 0         ['dot_4[0][0]']               \n",
      "                                                                                                  \n",
      " dot_5 (Dot)                 (None, 4, 4)                 0         ['activation_2[0][0]',        \n",
      "                                                                     'lstm_4[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_8 (Bat  (None, 4, 4)                 16        ['dot_5[0][0]']               \n",
      " chNormalization)                                                                                 \n",
      "                                                                                                  \n",
      " concatenate_2 (Concatenate  (None, 4, 8)                 0         ['batch_normalization_8[0][0]'\n",
      " )                                                                  , 'lstm_5[0][0]']             \n",
      "                                                                                                  \n",
      " time_distributed_2 (TimeDi  (None, 4, 1)                 9         ['concatenate_2[0][0]']       \n",
      " stributed)                                                                                       \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 361 (1.41 KB)\n",
      "Trainable params: 337 (1.32 KB)\n",
      "Non-trainable params: 24 (96.00 Byte)\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Load dataset metadata from JSON\n",
    "with open(f'{DATASET_PATH}/dataset32-MAESTRO-len{SAMPLE_LENGTH}.json', 'r') as f:\n",
    "    dataset_metadata = json.load(f)\n",
    "    train_time_diff_min, train_time_diff_max = dataset_metadata['train_time_diff_min'], dataset_metadata['train_time_diff_max']\n",
    "    note_num_min, note_num_max = dataset_metadata['note_num_min'], dataset_metadata['note_num_max']\n",
    "    note_num_diff_min, note_num_diff_max = dataset_metadata['note_num_diff_min'], dataset_metadata['note_num_diff_max']\n",
    "    length_min, length_max = dataset_metadata['length_min'], dataset_metadata['length_max']\n",
    "    velocity_min, velocity_max = dataset_metadata['velocity_min'], dataset_metadata['velocity_max']\n",
    "\n",
    "# Utility functions\n",
    "def divide_list(data, length, overlap=0):\n",
    "    for i in range(0, len(data) - length + 1, length - overlap):\n",
    "        yield data[i:i + length]\n",
    "    if len(data) % length != 0:\n",
    "        yield data[-(len(data) % length):]\n",
    "\n",
    "def pad_data(data, feature_num):\n",
    "    last_array = data.pop()\n",
    "    padding = np.zeros((SAMPLE_LENGTH - len(last_array), feature_num), dtype=FLOAT_DTYPE)\n",
    "    data.append(np.concatenate((last_array, padding)))\n",
    "    return data\n",
    "\n",
    "def make_dataset(csv_data, columns_train, columns_label):\n",
    "    data_input = np.array(csv_data[columns_train], dtype=FLOAT_DTYPE)\n",
    "    data_label = np.array(csv_data[columns_label], dtype=FLOAT_DTYPE)\n",
    "\n",
    "    # Normalize data based on metadata\n",
    "    data_input[:, 0] = (data_input[:, 0] - train_time_diff_min) / (train_time_diff_max - train_time_diff_min)\n",
    "    data_input[:, 1] = (data_input[:, 1] - note_num_min) / (note_num_max - note_num_min)\n",
    "    data_input[:, 2] = (data_input[:, 2] - length_min) / (length_max - length_min)\n",
    "    data_input[:, 3] = (data_input[:, 3] - note_num_diff_min) / (note_num_diff_max - note_num_diff_min)\n",
    "    data_label[:, 0] = (data_label[:, 0] - velocity_min) / (velocity_max - velocity_min)\n",
    "\n",
    "    dataset_input = pad_data(list(divide_list(data_input, SAMPLE_LENGTH)), FEATURE_NUM)\n",
    "    dataset_label = pad_data(list(divide_list(data_label, SAMPLE_LENGTH)), 1)\n",
    "    \n",
    "    return np.array(dataset_input, dtype=FLOAT_DTYPE), np.array(dataset_label, dtype=FLOAT_DTYPE)\n",
    "\n",
    "def generate_csv(csv_file, filename, columns_input, result):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    data_demo_input = np.array(csv_file[columns_input], dtype=int)\n",
    "    result = result[:len(data_demo_input)] if len(result) > len(data_demo_input) else result\n",
    "    data_demo_velocity = np.round(result).astype(int)\n",
    "    \n",
    "    pd.DataFrame({\n",
    "        'time': data_demo_input[:, 0],\n",
    "        'time_diff': data_demo_input[:, 1],\n",
    "        'note_num': data_demo_input[:, 2],\n",
    "        'length': data_demo_input[:, 3],\n",
    "        'note_num_diff': data_demo_input[:, 4],\n",
    "        'low_octave': data_demo_input[:, 5],\n",
    "        'velocity': data_demo_velocity\n",
    "    }).to_csv(filename, index=False)\n",
    "\n",
    "# Load CSV files\n",
    "csv_files_demo = []\n",
    "for filename in glob.glob(os.path.join(DATASET_PATH, '*.csv')):\n",
    "    df = pd.read_csv(filename, index_col=None, header=0)\n",
    "    df.attrs['filename'] = filename\n",
    "    csv_files_demo.append(df)\n",
    "\n",
    "# Load trained model\n",
    "def make_mse_cosine_loss(alpha):\n",
    "    def mse_cosine_loss(y_true, y_pred):\n",
    "        return alpha * (1 * cosine_similarity(y_true, y_pred)) + (1 - alpha) * mse(y_true, y_pred)\n",
    "    return mse_cosine_loss\n",
    "\n",
    "mse_cosine_loss = make_mse_cosine_loss(alpha=0.15)\n",
    "model = load_model(MODEL_PATH, custom_objects={'mse_cosine_loss': mse_cosine_loss})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:\n",
      "  - Input Dataset Path : converted_dataset/maestro-v3/test\n",
      "  - Output Prediction Path : converted_dataset/maestro-v3//test_predict\n",
      "  - Model Path : saved_models/mvi-v2-2023-07-20_13-00_56-h4-e5-mse_cosine_loss-alpha0.15-m0.60-LSTM-luong_attention-MAESTRO.h5\n",
      "\n",
      "Normalized Scores:\n",
      "  - Recall 10%       : 0.5376\n",
      "  - Recall 5%        : 0.2935\n",
      "  - MAE              : 0.1062\n",
      "  - MSE              : 0.0177\n",
      "  - SD of Abs Error  : 0.0782\n",
      "  - SD of Prediction : 0.0490\n",
      "  - F1 Score         : 0.4795\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recall_list, recall_5_list, f1_list, mae_list, mse_list, sd_list, sd_ae_list = [], [], [], [] ,[], [], []\n",
    "threshold = 0.1 * velocity_max     # 10% of max velocity\n",
    "threshold_5 = 0.05 * velocity_max  # 5% of max velocity\n",
    "\n",
    "for demo_input_csv in csv_files_demo:\n",
    "    demo_dataset_input, demo_dataset_label = make_dataset(demo_input_csv, columns_train, columns_label)\n",
    "    demo_dataset_result = model.predict(demo_dataset_input, verbose=0)\n",
    "\n",
    "    # Ground truth and predicted velocities\n",
    "    result_velocity = np.round(demo_dataset_result.reshape(-1) * velocity_max).clip(0, velocity_max)\n",
    "    true_velocity = demo_dataset_label.reshape(-1) * velocity_max\n",
    "    \n",
    "    # Normalized Velocity\n",
    "    rv, tv = result_velocity / 127, true_velocity / 127\n",
    "\n",
    "    # MAE and MSE\n",
    "    mae = np.mean(np.abs(rv - tv))\n",
    "    mse = np.mean((rv - tv) ** 2)\n",
    "    mae_list.append(mae)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "    # Standard deviation, Standard deviation of absolute errors, and F1 score\n",
    "    sd_rv, sd_true = np.std(rv), np.std(tv)\n",
    "    sd_list.append(sd_rv)\n",
    "\n",
    "    sd_ae = np.std(np.abs(rv - tv))\n",
    "    sd_ae_list.append(sd_ae)\n",
    "\n",
    "    f1 = 2 * ((sd_rv / sd_true) * (1 - (3 * mae))) / ((sd_rv / sd_true) + (1 - (3 * mae)))\n",
    "    f1_list.append(f1)\n",
    "\n",
    "    # Recall calculation based on 10% threshold\n",
    "    within_threshold = np.abs(result_velocity - true_velocity) <= threshold\n",
    "    tp = np.sum(within_threshold)\n",
    "    fn = fp = np.sum(~within_threshold)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    recall_list.append(recall)\n",
    "\n",
    "    # Recall calculation based on 5% threshold\n",
    "    tp, fn, fp = 0, 0, 0\n",
    "    within_5_threshold = np.abs(result_velocity - true_velocity) <= threshold_5\n",
    "    tp = np.sum(within_5_threshold)\n",
    "    fn = fp = np.sum(~within_5_threshold)\n",
    "    recall_5 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    recall_5_list.append(recall_5)\n",
    "\n",
    "    # Generate output CSV\n",
    "    original_filename = demo_input_csv.attrs['filename']\n",
    "    demo_output_csv_filename = f'{OUTPUT_PATH}/{Path(original_filename).stem}_predicted.csv'\n",
    "    generate_csv(demo_input_csv, demo_output_csv_filename, columns_full_input, result_velocity)\n",
    "\n",
    "# Summary evaluation output\n",
    "print(f\"Paths:\\n\"\n",
    "      f\"  - Input Dataset Path : {DATASET_PATH}\\n\"\n",
    "      f\"  - Output Prediction Path : {OUTPUT_PATH}\\n\"\n",
    "      f\"  - Model Path : {MODEL_PATH}\\n\\n\"\n",
    "      f\"Normalized Scores:\\n\"\n",
    "      f\"  - Recall 10%       : {np.mean(recall_list):.4f}\\n\"\n",
    "      f\"  - Recall 5%        : {np.mean(recall_5_list):.4f}\\n\"\n",
    "      f\"  - MAE              : {np.mean(mae_list):.4f}\\n\"\n",
    "      f\"  - MSE              : {np.mean(mse_list):.4f}\\n\"\n",
    "      f\"  - SD of Abs Error  : {np.mean(sd_ae_list):.4f}\\n\"\n",
    "      f\"  - SD of Prediction : {np.mean(sd_list):.4f}\\n\"\n",
    "      f\"  - F1 Score         : {np.mean(f1_list):.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:\n",
      "  - Input Dataset Path : converted_dataset/maestro-v3/test\n",
      "  - Output Prediction Path : converted_dataset/maestro-v3//test_predict\n",
      "  - Model Path : saved_models/mvi-v2-2023-07-20_13-00_56-h4-e5-mse_cosine_loss-alpha0.15-m0.60-LSTM-luong_attention-MAESTRO.h5\n",
      "\n",
      "Scores (Un-normalized):\n",
      "  - Recall 10%       : 0.5376\n",
      "  - Recall 5%        : 0.2935\n",
      "  - MAE              : 13.4817\n",
      "  - MSE              : 286.1327\n",
      "  - SD of Abs Error  : 9.9258\n",
      "  - SD of Prediction : 6.2266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "recall_list, recall_5_list, f1_list, mae_list, mse_list, sd_list, sd_ae_list = [], [], [], [] ,[], [], []\n",
    "threshold = 0.1 * velocity_max     # 10% of max velocity\n",
    "threshold_5 = 0.05 * velocity_max  # 5% of max velocity\n",
    "\n",
    "for demo_input_csv in csv_files_demo:\n",
    "    demo_dataset_input, demo_dataset_label = make_dataset(demo_input_csv, columns_train, columns_label)\n",
    "    demo_dataset_result = model.predict(demo_dataset_input, verbose=0)\n",
    "\n",
    "    # Ground truth and predicted velocities\n",
    "    result_velocity = np.round(demo_dataset_result.reshape(-1) * velocity_max).clip(0, velocity_max)\n",
    "    true_velocity = demo_dataset_label.reshape(-1) * velocity_max\n",
    "    \n",
    "    # De-Normalized Velocity\n",
    "    rv, tv = result_velocity, true_velocity\n",
    "\n",
    "    # MAE and MSE\n",
    "    mae = np.mean(np.abs(rv - tv))\n",
    "    mse = np.mean((rv - tv) ** 2)\n",
    "    mae_list.append(mae)\n",
    "    mse_list.append(mse)\n",
    "\n",
    "    # Standard deviation, Standard deviation of absolute errors, and F1 score\n",
    "    sd_rv, sd_true = np.std(rv), np.std(tv)\n",
    "    sd_list.append(sd_rv)\n",
    "\n",
    "    sd_ae = np.std(np.abs(rv - tv))\n",
    "    sd_ae_list.append(sd_ae)\n",
    "\n",
    "    f1 = 2 * ((sd_rv / sd_true) * (1 - (3 * mae))) / ((sd_rv / sd_true) + (1 - (3 * mae)))\n",
    "    f1_list.append(f1)\n",
    "\n",
    "    # Recall calculation based on 10% threshold\n",
    "    within_threshold = np.abs(result_velocity - true_velocity) <= threshold\n",
    "    tp = np.sum(within_threshold)\n",
    "    fn = fp = np.sum(~within_threshold)\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    recall_list.append(recall)\n",
    "\n",
    "    # Recall calculation based on 5% threshold\n",
    "    tp, fn, fp = 0, 0, 0\n",
    "    within_5_threshold = np.abs(result_velocity - true_velocity) <= threshold_5\n",
    "    tp = np.sum(within_5_threshold)\n",
    "    fn = fp = np.sum(~within_5_threshold)\n",
    "    recall_5 = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    recall_5_list.append(recall_5)\n",
    "\n",
    "    # Generate output CSV\n",
    "    original_filename = demo_input_csv.attrs['filename']\n",
    "    demo_output_csv_filename = f'{OUTPUT_PATH}/{Path(original_filename).stem}_predicted.csv'\n",
    "    generate_csv(demo_input_csv, demo_output_csv_filename, columns_full_input, result_velocity)\n",
    "\n",
    "# Summary evaluation output\n",
    "print(f\"Paths:\\n\"\n",
    "      f\"  - Input Dataset Path : {DATASET_PATH}\\n\"\n",
    "      f\"  - Output Prediction Path : {OUTPUT_PATH}\\n\"\n",
    "      f\"  - Model Path : {MODEL_PATH}\\n\\n\"\n",
    "      f\"Scores (Un-normalized):\\n\"\n",
    "      f\"  - Recall 10%       : {np.mean(recall_list):.4f}\\n\"\n",
    "      f\"  - Recall 5%        : {np.mean(recall_5_list):.4f}\\n\"\n",
    "      f\"  - MAE              : {np.mean(mae_list):.4f}\\n\"\n",
    "      f\"  - MSE              : {np.mean(mse_list):.4f}\\n\"\n",
    "      f\"  - SD of Abs Error  : {np.mean(sd_ae_list):.4f}\\n\"\n",
    "      f\"  - SD of Prediction : {np.mean(sd_list):.4f}\\n\")\n",
    "    #   f\"  - F1 Score         : {np.mean(f1_list):.4f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
