{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle as pkl\n",
    "import os, glob, json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train time_diff range: 0.0 to 37568.0\n",
      "Length range: 1.0 to 46778.0\n",
      "Note num diff range: -81.0 to 85.0\n"
     ]
    }
   ],
   "source": [
    "# Select dataset\n",
    "dataset_name = 'MAESTRO' # 'SMD', 'MAESTRO'\n",
    "FLOAT_DTYPE = np.float32 # np.float64\n",
    "AMOUNT, root_path, dataset_train_path, dataset_val_path, dataset_test_path = '','','','',''\n",
    "\n",
    "if dataset_name == 'MAESTRO':\n",
    "    AMOUNT = '' # select from '' (entire dataset) | '-medium'| '-small' \n",
    "    root_path = '/media/datadisk/home/22828187/zhanh/vae/ablation_study/Kim2023_model/converted_dataset/maestro-v3'\n",
    "    dataset_train_path = f'{root_path}/train'\n",
    "    dataset_val_path = f'{root_path}/validation'\n",
    "    dataset_test_path = f'{root_path}/test'\n",
    "\n",
    "# elif dataset_name == 'GiantMIDIPiano':\n",
    "#     root_path = '/home/tikim/dataset/giantmidi'\n",
    "#     dataset_train_path = f'{root_path}/dataset/train'\n",
    "#     dataset_val_path = f'{root_path}/dataset/validation'\n",
    "#     dataset_test_path = f'{root_path}/dataset/test'\n",
    "\n",
    "# elif dataset_name == 'chien2021': \n",
    "#     # Also known as Piano-e-competition dataset, some MIDI duplicate with Maestro dataset\n",
    "#     root_path = '/home/tikim/dataset/chien2021'\n",
    "#     dataset_train_path = f'{root_path}/dataset/train'\n",
    "#     dataset_test_path = f'{root_path}/dataset/test'\n",
    "\n",
    "elif dataset_name == 'SMD':\n",
    "    # Set up SMD paths (this dataset is a test set only)\n",
    "    root_path = \"/media/datadisk/home/22828187/zhanh/vae/ablation_study/Kim2023_model/converted_dataset/SMD\"\n",
    "    dataset_test_path = f'{root_path}/test\n",
    "\n",
    "\n",
    "# Initialize data variables\n",
    "extension = 'csv'\n",
    "note_num_min, note_num_max = 0, 127\n",
    "velocity_min, velocity_max = 0, 127\n",
    "dataset_entire_train = None # Reset\n",
    "dataset_entire_test = None # Reset\n",
    "\n",
    "# Load training data (if applicable)\n",
    "if dataset_name in ['MAESTRO', 'GiantMIDIPiano', 'chien2021']:\n",
    "    csv_files_train = []\n",
    "    if os.path.exists(dataset_train_path):\n",
    "        os.chdir(dataset_train_path)\n",
    "        train_csv_filenames = glob.glob(f'*.{extension}')\n",
    "        for filename in train_csv_filenames:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            if len(df) == 0:\n",
    "                print(f'Dataframe is empty! {filename} skipping...')\n",
    "                continue\n",
    "            csv_files_train.append(df)\n",
    "        if csv_files_train:\n",
    "            dataset_entire_train = pd.concat(csv_files_train, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            print(\"No CSV files found in the training dataset directory.\")\n",
    "    else:\n",
    "        print(\"Training directory does not exist.\")\n",
    "\n",
    "\n",
    "# Load testing data\n",
    "csv_files_test = []\n",
    "if os.path.exists(dataset_test_path):\n",
    "    os.chdir(dataset_test_path)\n",
    "    test_csv_filenames = glob.glob(f'*.{extension}')\n",
    "    for filename in test_csv_filenames:\n",
    "        df = pd.read_csv(filename, index_col=None, header=0)\n",
    "        if len(df) == 0:\n",
    "            print(f\"Dataframe is empty! {filename} skipping...\")\n",
    "            continue\n",
    "        csv_files_test.append(df)\n",
    "    if csv_files_test:\n",
    "        dataset_entire_test = pd.concat(csv_files_test, axis=0, ignore_index=True)\n",
    "    else:\n",
    "        print(\"No CSV files found in the testing dataset directory.\")\n",
    "else:\n",
    "    print(\"Testing directory does not exist.\")\n",
    "\n",
    "\n",
    "# Load validation data (if applicable)\n",
    "if dataset_name in ['MAESTRO', 'GiantMIDIPiano']:\n",
    "    csv_files_val = []\n",
    "    if os.path.exists(dataset_val_path):\n",
    "        os.chdir(dataset_val_path)\n",
    "        val_csv_filenames = glob.glob(f'*.{extension}')\n",
    "        for filename in val_csv_filenames:\n",
    "            df = pd.read_csv(filename, index_col=None, header=0)\n",
    "            if len(df) == 0:\n",
    "                print(f'Dataframe is empty! {filename} skipping...')\n",
    "                continue\n",
    "            csv_files_val.append(df)\n",
    "        if csv_files_val:\n",
    "            dataset_entire_val = pd.concat(csv_files_val, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            print(\"No CSV files found in the validation dataset directory.\")\n",
    "    else:\n",
    "        print(\"Validation directory does not exist.\")\n",
    "\n",
    "\n",
    "# Perform processing on train data\n",
    "if dataset_name != \"SMD\": \n",
    "    columns_train = ['time_diff', 'note_num', 'length', 'note_num_diff', 'low_octave']\n",
    "    columns_label = ['velocity']\n",
    "    dataset_entire_train = np.array(dataset_entire_train[columns_train], dtype=FLOAT_DTYPE)\n",
    "    time_diff_min = np.min(dataset_entire_train[:, 0])\n",
    "    time_diff_max = np.max(dataset_entire_train[:, 0])\n",
    "    length_min = np.min(dataset_entire_train[:, 2])\n",
    "    length_max = np.max(dataset_entire_train[:, 2])\n",
    "    note_num_diff_min = np.min(dataset_entire_train[:, 3])\n",
    "    note_num_diff_max = np.max(dataset_entire_train[:, 3])\n",
    "    print(f\"Train time_diff range: {time_diff_min} to {time_diff_max}\")\n",
    "    print(f\"Length range: {length_min} to {length_max}\")\n",
    "    print(f\"Note num diff range: {note_num_diff_min} to {note_num_diff_max}\")\n",
    "# Perform processing on test data\n",
    "else: \n",
    "    columns_test = ['time_diff', 'note_num', 'length', 'note_num_diff', 'low_octave']\n",
    "    columns_label = ['velocity']\n",
    "    dataset_entire_test = np.array(dataset_entire_test[columns_test], dtype=FLOAT_DTYPE)\n",
    "    time_diff_min = np.min(dataset_entire_test[:, 0])\n",
    "    time_diff_max = np.max(dataset_entire_test[:, 0])\n",
    "    length_min = np.min(dataset_entire_test[:, 2])\n",
    "    length_max = np.max(dataset_entire_test[:, 2])\n",
    "    note_num_diff_min = np.min(dataset_entire_test[:, 3])\n",
    "    note_num_diff_max = np.max(dataset_entire_test[:, 3])\n",
    "    print(f\"Test time_diff range: {time_diff_min} to {time_diff_max}\")\n",
    "    print(f\"Length range: {length_min} to {length_max}\")\n",
    "    print(f\"Note num diff range: {note_num_diff_min} to {note_num_diff_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_list(l, n, overlapping_window=0):\n",
    "    for i in range(0, len(l) - n + 1, n - overlapping_window):\n",
    "        yield l[i:i + n]\n",
    "    if len(l) % n != 0 and len(l) % n < n:\n",
    "        yield l[-(len(l) % n):]\n",
    "                \n",
    "SAMPLE_LENGTH = 4\n",
    "FEATURE_NUM = 5\n",
    "def pad_data(data, feature_num):\n",
    "    # print(f'Length of data: {len(data)}')\n",
    "    if len(data[-1]) != SAMPLE_LENGTH:\n",
    "        # print(f'Length of last array: {len(data[-1])}')\n",
    "        last_array = data.pop()\n",
    "        # print(f'before padding: {last_array}')\n",
    "        zero_array = np.zeros((SAMPLE_LENGTH - len(last_array), feature_num), dtype=FLOAT_DTYPE)\n",
    "        last_array = np.concatenate((last_array, zero_array))\n",
    "        # print(f'after padding: {last_array}')\n",
    "        data.append(last_array)\n",
    "        # print(f'Length of last array (after padding): {len(data[-1])}')\n",
    "    return data\n",
    "\n",
    "def make_dataset(csv_files, columns_train, columns_label):\n",
    "    dataset_entire_input = np.empty((0, SAMPLE_LENGTH, FEATURE_NUM), dtype=FLOAT_DTYPE)\n",
    "    dataset_entire_label = np.empty((0, SAMPLE_LENGTH, 1), dtype=FLOAT_DTYPE)\n",
    "    \n",
    "    for df in csv_files:\n",
    "        data_input_raw = np.array(df[columns_train], dtype=FLOAT_DTYPE)\n",
    "        data_label_raw = np.array(df[columns_label], dtype=FLOAT_DTYPE)\n",
    "        \n",
    "        # normalize the time difference\n",
    "        data_input_raw[:, 0] = (data_input_raw[:, 0] - time_diff_min) / (time_diff_max - time_diff_min)\n",
    "        # normalize the note number\n",
    "        data_input_raw[:, 1] = (data_input_raw[:, 1] - note_num_min) / (note_num_max - note_num_min)\n",
    "        # normalize the length\n",
    "        data_input_raw[:, 2] = (data_input_raw[:, 2] - length_min) / (length_max - length_min)\n",
    "        # normalize the note number difference\n",
    "        data_input_raw[:, 3] = (data_input_raw[:, 3] - note_num_diff_min) / (note_num_diff_max - note_num_diff_min)\n",
    "        # you don't have to normalize the low octave\n",
    "        # normalize the time\n",
    "        # data_input_raw[:, 5] = (data_input_raw[:, 5] - time_min) / (time_max - time_min)\n",
    "        \n",
    "        # normalize the velocity\n",
    "        data_label_raw[:, 0] = (data_label_raw[:, 0] - velocity_min) / (velocity_max - velocity_min)\n",
    "\n",
    "        data_input_raw2 = list(divide_list(data_input_raw, SAMPLE_LENGTH, SAMPLE_LENGTH - 1))\n",
    "        data_input_raw2 = pad_data(data_input_raw2, FEATURE_NUM)\n",
    "        data_input = np.array(data_input_raw2, dtype=FLOAT_DTYPE)\n",
    "        dataset_entire_input = np.vstack((dataset_entire_input, data_input))\n",
    "\n",
    "        data_label_raw2 = list(divide_list(data_label_raw, SAMPLE_LENGTH, SAMPLE_LENGTH - 1))\n",
    "        data_label_raw2 = pad_data(data_label_raw2, 1)\n",
    "        data_label = np.array(data_label_raw2, dtype=FLOAT_DTYPE)\n",
    "        dataset_entire_label = np.vstack((dataset_entire_label, data_label))\n",
    "    \n",
    "    return dataset_entire_input, dataset_entire_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (dataset_name == 'MAESTRO' or dataset_name == 'GiantMIDIPiano'):\n",
    "    dataset_train_input, dataset_train_label = make_dataset(csv_files_train, columns_train, columns_label)\n",
    "    dataset_val_input, dataset_val_label = make_dataset(csv_files_val, columns_train, columns_label)\n",
    "    dataset_test_input, dataset_test_label = make_dataset(csv_files_test, columns_train, columns_label)\n",
    "\n",
    "# elif (dataset_name == 'chien2021'):\n",
    "#     dataset_train_input, dataset_train_label = make_dataset(csv_files_train, columns_train, columns_label)\n",
    "#     dataset_test_input, dataset_test_label = make_dataset(csv_files_test, columns_train, columns_label)\n",
    "\n",
    "elif (dataset_name == 'SMD'):\n",
    "    dataset_test_input, dataset_test_label = make_dataset(csv_files_test, columns_test, columns_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_filename_short = ''\n",
    "dataset_filename = ''\n",
    "\n",
    "\n",
    "if dataset_name == 'MAESTRO':\n",
    "    if(FLOAT_DTYPE == np.float32):\n",
    "        dataset_filename_short = f'dataset32-{dataset_name}-len{SAMPLE_LENGTH}{AMOUNT}'\n",
    "    elif (FLOAT_DTYPE == np.float64):\n",
    "        dataset_filename_short = f'dataset64-{dataset_name}-len{SAMPLE_LENGTH}{AMOUNT}'\n",
    "    else:\n",
    "        dataset_filename_short = 'dataset'\n",
    "\n",
    "elif dataset_name in ['GiantMIDIPiano', 'chien2021', 'SMD']:\n",
    "    if (FLOAT_DTYPE == np.float32):\n",
    "        dataset_filename_short = f'dataset32-{dataset_name}-len{SAMPLE_LENGTH}'\n",
    "    elif (FLOAT_DTYPE == np.float64):\n",
    "        dataset_filename_short = f'dataset64-{dataset_name}-len{SAMPLE_LENGTH}'\n",
    "    else:\n",
    "        dataset_filename_short = 'dataset'\n",
    "\n",
    "\n",
    "dataset_filename = f'{dataset_filename_short}.pkl'\n",
    "\n",
    "\n",
    "if (dataset_name == 'MAESTRO' or dataset_name == 'GiantMIDIPiano'):\n",
    "    pkl.dump({'dataset_train_input': dataset_train_input, 'dataset_train_label': dataset_train_label,\n",
    "            'dataset_val_input': dataset_val_input, 'dataset_val_label': dataset_val_label,\n",
    "            'dataset_test_input': dataset_test_input, 'dataset_test_label': dataset_test_label,\n",
    "            'train_time_diff_min': time_diff_min, 'train_time_diff_max': time_diff_max, \n",
    "            'note_num_min': note_num_min, 'note_num_max': note_num_max, \n",
    "            'note_num_diff_min': note_num_diff_min, 'note_num_diff_max': note_num_diff_max,\n",
    "            'length_min': length_min, 'length_max': length_max,\n",
    "            # 'time_min': time_min, 'time_max': time_max,\n",
    "            'velocity_min': velocity_min, 'velocity_max': velocity_max}, open(dataset_filename, 'wb'))\n",
    "    \n",
    "elif (dataset_name == 'chien2021'):\n",
    "    pkl.dump({'dataset_train_input': dataset_train_input, 'dataset_train_label': dataset_train_label,\n",
    "            'dataset_test_input': dataset_test_input, 'dataset_test_label': dataset_test_label,\n",
    "            'train_time_diff_min': time_diff_min, 'train_time_diff_max': time_diff_max,\n",
    "            'note_num_min': note_num_min, 'note_num_max': note_num_max,\n",
    "            'note_num_diff_min': note_num_diff_min, 'note_num_diff_max': note_num_diff_max,\n",
    "            'length_min': length_min, 'length_max': length_max,\n",
    "            # 'time_min': time_min, 'time_max': time_max,\n",
    "            'velocity_min': velocity_min, 'velocity_max': velocity_max}, open(dataset_filename, 'wb'))\n",
    "    \n",
    "elif (dataset_name == 'SMD'):\n",
    "    pkl.dump({'dataset_test_input': dataset_test_input, 'dataset_test_label': dataset_test_label,\n",
    "            'train_time_diff_min': time_diff_min, 'train_time_diff_max': time_diff_max,\n",
    "            'note_num_min': note_num_min, 'note_num_max': note_num_max,\n",
    "            'note_num_diff_min': note_num_diff_min, 'note_num_diff_max': note_num_diff_max,\n",
    "            'length_min': length_min, 'length_max': length_max,\n",
    "            # 'time_min': time_min, 'time_max': time_max,\n",
    "            'velocity_min': velocity_min, 'velocity_max': velocity_max}, open(dataset_filename, 'wb'))\n",
    "\n",
    "\n",
    "# save metadata to json. Convert float32 to float\n",
    "metadata = {'train_time_diff_min': time_diff_min, 'train_time_diff_max': time_diff_max,\n",
    "            'note_num_min': note_num_min, 'note_num_max': note_num_max,\n",
    "            'note_num_diff_min': note_num_diff_min, 'note_num_diff_max': note_num_diff_max,\n",
    "            'length_min': length_min, 'length_max': length_max,\n",
    "            # 'time_min': time_min, 'time_max': time_max,\n",
    "            'velocity_min': velocity_min, 'velocity_max': velocity_max}\n",
    "\n",
    "\n",
    "with open(f'{dataset_filename_short}.json', 'w') as f:\n",
    "    json.dump(metadata, f, default=int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2p39",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
